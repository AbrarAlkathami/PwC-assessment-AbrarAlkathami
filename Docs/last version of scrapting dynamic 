from pinecone import Pinecone
from sentence_transformers import SentenceTransformer
import os
from langchain_pinecone import PineconeVectorStore
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain_openai import OpenAIEmbeddings,OpenAI
from langchain.schema import Document
from langchain.chains.question_answering import load_qa_chain
from langchain_openai import ChatOpenAI 
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings

os.environ['OPENAI_API_KEY'] = "sk-proj-t3quEkktCcLSjpmFpUKNT3BlbkFJSyeBPFQno4NVMSw3P1BP"
os.environ['PINECONE_API_KEY'] = '085005dc-bd04-45c5-9636-09dfc1c13a79'


# Function to read files and create chunks
def read_files_and_create_chunks(directory, chunk_size=800, chunk_overlap=50):
    counter=0
    documents = []
    # Read the content of pages that stored in files 
    for filename in os.listdir(directory):
        filepath = os.path.join(directory, filename)
        with open(filepath, 'r', encoding='utf-8') as file:
            text = file.read()

            doc_obj = Document(page_content=text)

            # Divied the file content into chunks
            text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            doc=text_splitter.split_documents([doc_obj])
            documents.extend(doc)
            
    return documents

def store_vectordb():
    # To create chunks
    folderPath=Path("ScraptingData")
    documents = read_files_and_create_chunks(folderPath)
    print(len(documents))

    # To create embeddings for each chunk
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    print(embeddings)

    # convert text to vectors
    vectors= embeddings.embed_query("How Are You?")
    # To create embeddings for each chunk
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    print(embeddings)
    # Vector Search DB in Pinecone
    index_name = "data"

    index= PineconeVectorStore.from_documents(
            documents,
            index_name=index_name,
            embedding=embeddings
        )
    print("finished embedding")
    return index

#cosine similarity ret results from vector DB
def retrieve_query(query,k=2):
    # To create embeddings for each chunk
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    print(embeddings)
    # Vector Search DB in Pinecone
    index_name = "data"
    vectorstore = PineconeVectorStore(index_name=index_name, embedding=HuggingFaceEmbeddings()) 
    matching_results= vectorstore.similarity_search(query,k=k)
    return matching_results


gpt3_llm = ChatOpenAI(  
    model_name='gpt-3.5-turbo',  
    temperature=0.5 
)  
# gpt4_llm = ChatOpenAI(  
#     model_name='gpt-4',  
#     temperature=0.5 
# )  
# Llama2_llm = ChatOpenAI(  
#     model_name='gpt-3.5-turbo',  
#     temperature=0.5 
# )  
# Falcon40b_llm = ChatOpenAI(  
#     model_name='gpt-3.5-turbo',  
#     temperature=0.5 
# )  

gpt3_chain= load_qa_chain(gpt3_llm,chain_type="stuff")
# gpt4_chain= load_qa_chain(gpt4_llm,chain_type="stuff")
# Llama2_l= load_qa_chain(Llama2_llm,chain_type="stuff")
# Falcon40b_chain= load_qa_chain(Falcon40b_llm,chain_type="stuff")

# Search for the answer from vector DB
def retrieve_answers(query):
    doc_search= retrieve_query(query)
    print(doc_search)
    response=gpt3_chain.run(input_document=doc_search, question=query)
    return response


our_query="Can you name all the eServices?"
answer= retrieve_answers(our_query)
print(answer)